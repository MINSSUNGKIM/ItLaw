import os
import json
import re
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np

class LegalDocumentProcessor:
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        """ë²•ë¥  ë¬¸ì„œ ì²˜ë¦¬ í´ë˜ìŠ¤ ì´ˆê¸°í™”"""
        print("SentenceTransformer ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = SentenceTransformer(model_name)
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def extract_text_from_docx(self, file_path):
        """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            document = Document(file_path)
            text = ""
            for paragraph in document.paragraphs:
                if paragraph.text.strip():
                    text += paragraph.text.strip() + "\n"
            return text
        except Exception as e:
            print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def preprocess_legal_text(self, text):
        """ë²•ë¥  í…ìŠ¤íŠ¸ë¥¼ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        # "ì œNì¡°" íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        chunks = re.split(r'(ì œ\d+ì¡°(?:\([^)]*\))?)', text)
        
        result_chunks = []
        current_chunk = ""
        
        for i, part in enumerate(chunks):
            # "ì œNì¡°" íŒ¨í„´ê³¼ ë§¤ì¹­ë˜ëŠ” ê²½ìš°
            if re.match(r'ì œ\d+ì¡°(?:\([^)]*\))?', part):
                # ì´ì „ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
                if current_chunk.strip():
                    result_chunks.append(current_chunk.strip())
                current_chunk = part  # ìƒˆ ì¡°ë¬¸ ì‹œì‘
            else:
                current_chunk += " " + part
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk.strip():
            result_chunks.append(current_chunk.strip())
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        cleaned_chunks = []
        for chunk in result_chunks:
            # ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±°
            chunk = re.sub(r'\[[^\]]*\]', '', chunk)  # [ë‚´ìš©] ì œê±°
            chunk = re.sub(r'\<[^>]*\>', '', chunk)   # <ë‚´ìš©> ì œê±°
            chunk = re.sub(r'ì œ\d+ì¥[^\n]*', '', chunk)  # ì¥ ì œëª© ì œê±°
            chunk = re.sub(r'ì œ\d+ì ˆ[^\n]*', '', chunk)  # ì ˆ ì œëª© ì œê±°
            
            # ê³µë°± ì •ë¦¬
            chunk = re.sub(r'\s+', ' ', chunk).strip()
            
            # ìµœì†Œ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œê±°)
            if len(chunk) > 10:
                cleaned_chunks.append(chunk)
        
        return cleaned_chunks
    
    def create_embeddings(self, chunks):
        """í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜"""
        print(f"{len(chunks)}ê°œ ì²­í¬ ë²¡í„°í™” ì¤‘...")
        embeddings = self.model.encode(chunks, convert_to_tensor=False, show_progress_bar=True)
        return embeddings
    
    def save_to_json(self, chunks, embeddings, output_path):
        """ì²­í¬ì™€ ë²¡í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if len(chunks) != len(embeddings):
            raise ValueError("ì²­í¬ ìˆ˜ì™€ ì„ë² ë”© ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        
        # ë°ì´í„° êµ¬ì¡° ìƒì„±
        data = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            data.append({
                "id": i,
                "text": chunk,
                "vector": embedding.tolist(),
                "metadata": {
                    "length": len(chunk),
                    "article_pattern": bool(re.search(r'ì œ\d+ì¡°', chunk))
                }
            })
        
        # JSON ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(data)}ê°œ ì²­í¬ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return len(data)
    
    def process_document(self, input_file, output_file):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=" * 50)
        print("ë²•ë¥  ë¬¸ì„œ RAG ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        print("=" * 50)
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print("1. ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        raw_text = self.extract_text_from_docx(input_file)
        if not raw_text:
            return False
        
        # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹
        print("2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹ ì¤‘...")
        chunks = self.preprocess_legal_text(raw_text)
        print(f"   ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # 3. ë²¡í„°í™”
        print("3. í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")
        embeddings = self.create_embeddings(chunks)
        
        # 4. JSON ì €ì¥
        print("4. ê²°ê³¼ ì €ì¥ ì¤‘...")
        saved_count = self.save_to_json(chunks, embeddings, output_file)
        
        print("=" * 50)
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! {saved_count}ê°œ ë²•ì¡°ë¬¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 50)
        
        return True
    
    def preview_chunks(self, chunks, num_preview=3):
        """ì²­í¬ ë¯¸ë¦¬ë³´ê¸°"""
        print(f"\nğŸ“‹ ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (ì´ {len(chunks)}ê°œ ì¤‘ {min(num_preview, len(chunks))}ê°œ):")
        print("-" * 50)
        for i, chunk in enumerate(chunks[:num_preview]):
            print(f"ì²­í¬ {i+1}:")
            print(f"{chunk[:200]}{'...' if len(chunk) > 200 else ''}")
            print("-" * 50)

def main():
    # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
    input_file = "law.docx"  # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    output_file = "legal_documents.json"  # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    
    # ì²˜ë¦¬ ê°ì²´ ìƒì„±
    processor = LegalDocumentProcessor()
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(input_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        print("íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return
    
    # ë¬¸ì„œ ì²˜ë¦¬ ì‹¤í–‰
    success = processor.process_document(input_file, output_file)
    
    if success:
        print(f"ğŸ‰ ë‹¤ìŒ ë‹¨ê³„: {output_file}ì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì„¸ìš”!")
    else:
        print("âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
